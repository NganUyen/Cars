# -*- coding: utf-8 -*-
"""Features Engineering

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SQ90rQecLOz4mhYZh_-VM4dkv_8032Xy
"""

# ======================================================

import pandas as pd
import numpy as np
from scipy.stats import skew
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import StandardScaler, KBinsDiscretizer
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel
import xgboost as xgb
import warnings
warnings.filterwarnings("ignore")

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for f in filenames:
        print(os.path.join(dirname, f))

csv_path = "//content/cleaned_data_part_2_official.csv"
df = pd.read_csv(csv_path)
print(f"Loaded: {df.shape[0]} rows × {df.shape[1]} cols\n")

"""# CATEGORICAL ENCODING"""

# One-Hot: low cardinality (trừ Make)
low_card = ['Engine_Layout', 'Fuel_Type_Cleaned']
df = pd.get_dummies(df, columns=low_card, drop_first=True)
print(f"One-Hot done (excluding Make) → {df.shape[1]} cols")

# Target Encoding: Make (hãng xe, quan hệ phi tuyến tính)
print("Target Encoding for Make...")

kf = KFold(n_splits=5, shuffle=True, random_state=42)
df['Make_Encoded'] = np.nan

for train_idx, val_idx in kf.split(df):
    train_fold = df.iloc[train_idx]
    val_fold = df.iloc[val_idx]
    mean_map = train_fold.groupby('Make')['Price_USD'].mean()
    df.loc[val_idx, 'Make_Encoded'] = val_fold['Make'].map(mean_map)

df['Make_Encoded'].fillna(df['Price_USD'].mean(), inplace=True)
print("Target Encoding done → added 'Make_Encoded' column")

"""# NUMERICAL LOG and SCALE"""

num_cols = ['CC_Capacity', 'Horsepower', 'Top_Speed_kmh',
            'Acceleration_0_100_sec', 'Price_USD', 'Seats', 'Torque_Nm']

# Log transform skewed
skewed = df[num_cols].apply(skew, nan_policy='omit')
skewed_cols = skewed[abs(skewed) > 1].index.tolist()
print(f"Log1p applied to: {skewed_cols}")

for col in skewed_cols:
    df[f"{col}_log"] = np.log1p(df[col])
df.drop(columns=skewed_cols, inplace=True)

# Scale all numeric
scale_cols = [c for c in df.columns if c.endswith('_log') or c in ['Seats']]
scaler = StandardScaler()
df[scale_cols] = scaler.fit_transform(df[scale_cols])
print(f"Scaled {len(scale_cols)} cols → mean=0, std=1\n")

"""# DATETIME FEATURES"""

if 'Year' in df.columns:
    current_year = 2025
    df['Car_Age'] = current_year - df['Year']
    df['Is_New'] = (df['Car_Age'] <= 2).astype(int)
    print("Added: Car_Age, Is_New")
else:
    print("No 'Year' column → skipped datetime features")

"""# INTERACTION & RATIO FEATURES"""

# Interaction: Horsepower * Torque (actual power output)
if 'Horsepower_log' in df.columns and 'Torque_Nm_log' in df.columns:
    df['Power_Torque_Interact'] = df['Horsepower_log'] * df['Torque_Nm_log']

# Ratio: Power-to-Weight (if weight is available – assuming not → skip)
# df['HP_per_Ton'] = df['Horsepower_log'] / df['Weight_kg_log']

# Difference: Speed - Acceleration (acceleration capability relative to top speed)
if 'Top_Speed_kmh_log' in df.columns and 'Acceleration_0_100_sec_log' in df.columns:
    df['Speed_Accel_Diff'] = df['Top_Speed_kmh_log'] - df['Acceleration_0_100_sec_log']

print(f"Added interaction features → {df.shape[1]} columns\n")

"""# FEATURE SELECTION"""

# Define target column
target_col = 'Price_USD_log' if 'Price_USD_log' in df.columns else 'Price_USD'
y = df[target_col]
X = df.drop(columns=[target_col])

# Drop object columns (string types)
# Keep Make_Encoded because it's numeric
object_cols = X.select_dtypes(include=['object']).columns.tolist()
if len(object_cols) > 0:
    print(f"Dropping {len(object_cols)} object columns: {object_cols}")
    X = X.drop(columns=object_cols)

print(f"After cleaning: {X.shape[1]} numeric columns left")

# Train-test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train XGBoost for feature importance
import xgboost as xgb

model = xgb.XGBRegressor(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    tree_method='hist'
)

model.fit(X_train, y_train)

# Compute feature importance
importances = pd.Series(model.feature_importances_, index=X.columns)
top_features = importances.nlargest(30).index.tolist()

print(f"Selected {len(top_features)} top features (out of {X.shape[1]})")
print("Top 5 important features:", top_features[:5])

# Create final dataset for modeling
df_selected = df[top_features + [target_col]].copy()
print(f"Final dataset shape: {df_selected.shape}")

# Save final data
final_file = "FINAL_FEATURE_ENGINEERED_DATA.csv"
df_selected.to_csv(final_file, index=False)

print(f"Saved: {final_file}")
print(f"Target column: {target_col}")
print("Ready for modeling!")

final_file = "FINAL_FEATURE_ENGINEERED_DATA.csv"
df_selected.to_csv(final_file, index=False)
print(f"\nFINAL SAVE DONE!")
print(f"   File : {final_file}")
print(f"   Shape: {df_selected.shape}")
print(f"   Target: {y.name}")
print("   READY FOR FINAL MODELING!")